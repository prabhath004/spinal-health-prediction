{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66586cee-9e06-4dd2-9fce-f639c3eef106",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     confusion_matrix, classification_report, accuracy_score, precision_recall_curve, \n\u001b[1;32m      8\u001b[0m     roc_curve, auc, ConfusionMatrixDisplay\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m  \u001b[38;5;66;03m# For displaying DataFrame results in Jupyter Notebook\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# üìå **Load the saved models and scaler**\u001b[39;00m\n\u001b[1;32m     13\u001b[0m voting_clf \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoting_classifier.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score, precision_recall_curve, \n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "import ace_tools as tools  # For displaying DataFrame results in Jupyter Notebook\n",
    "\n",
    "# üìå **Load the saved models and scaler**\n",
    "voting_clf = joblib.load(\"voting_classifier.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# üìå **Load Predictions Data**\n",
    "df_results = pd.read_csv(\"spine_health_predictions.csv\")  # Load saved predictions\n",
    "df_results[\"Binary Prediction\"] = df_results[\"Final Prediction\"].apply(lambda x: 1 if x == \"Abnormal\" else 0)\n",
    "\n",
    "# üìå **Simulated Ground Truth (For Analysis Only)**\n",
    "# Assuming 50% Normal, 50% Abnormal for evaluation (Replace with real labels if available)\n",
    "y_test = np.array([0] * 50 + [1] * 50)  \n",
    "y_pred = df_results[\"Binary Prediction\"].values\n",
    "y_probs = df_results[\"ML Probability (Abnormal)\"].values\n",
    "\n",
    "# üìå **Compute & Plot Confusion Matrix**\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal\", \"Abnormal\"])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix for Spine Health Prediction\")\n",
    "plt.show()\n",
    "\n",
    "# üìå **Precision-Recall Curve**\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precisions[:-1], \"b-\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"r-\", label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall Tradeoff\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# üìå **ROC Curve & AUC Score**\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\", color=\"blue\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# üìå **Accuracy Distribution of ML Probabilities**\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df_results[\"ML Probability (Abnormal)\"], bins=20, kde=True, color=\"skyblue\")\n",
    "plt.axvline(x=0.6136, color='r', linestyle='dashed', label=\"Optimal Threshold (0.6136)\")\n",
    "plt.xlabel(\"ML Probability of Abnormality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Abnormality Predictions\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# üìå **Generate Classification Report**\n",
    "print(\"\\nüîç Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# üìå **Display Predictions Summary Table**\n",
    "tools.display_dataframe_to_user(name=\"Spine Prediction Results\", dataframe=df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a6292-6608-4839-bc89-fe2bd6d3e051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
